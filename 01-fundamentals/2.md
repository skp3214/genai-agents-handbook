# Introduction to Large Language Models (LLMs)

## Table of Contents

1. [What is a Language Model?](#what-is-a-language-model)
2. [What Makes a Language Model "Large"?](#what-makes-a-language-model-large)
3. [Key Architecture: Transformers](#key-architecture-transformers)
   1. [How Transformers Work: Attention Mechanism](#how-transformers-work-attention-mechanism)
   2. [Encoder-Decoder Structure](#encoder-decoder-structure)
4. [Types of Large Language Models](#types-of-large-language-models)
   1. [Encoder-Only Models](#encoder-only-models)
   2. [Decoder-Only Models](#decoder-only-models)
   3. [Encoder-Decoder Models](#encoder-decoder-models)
5. [How LLMs Are Trained](#how-llms-are-trained)
   1. [Pre-training](#pre-training)
   2. [Fine-tuning and Instruction Tuning](#fine-tuning-and-instruction-tuning)

## What is a Language Model?

A language model is a machine learning model designed to understand, predict, and generate human-like text. It learns the statistical patterns of language—such as grammar, semantics, and context—from vast amounts of text data. The core task during training is usually **next-token prediction**: given a sequence of words (tokens), the model predicts the most likely next token.

## What Makes a Language Model "Large"?

The term "Large Language Model" (LLM) is somewhat informal, but it generally refers to models with **billions of parameters**. 

- **Parameters** are the trainable weights in the neural network that the model adjusts during training to capture patterns in data.
- Early "large" models like BERT (2018) had around 110–340 million parameters.
- Modern LLMs, such as GPT-3 (175 billion), PaLM 2 (up to 540 billion), or Llama 3 (up to 405 billion), are orders of magnitude bigger.

"Largeness" can also refer to the massive scale of training data (often trillions of tokens from books, websites, code, etc.) and the computational resources required.

## Key Architecture: Transformers

The breakthrough that enabled modern LLMs was the **Transformer** architecture, introduced in the 2017 paper *"Attention Is All You Need"*.

### How Transformers Work: Attention Mechanism

Traditional recurrent models (like LSTMs) processed text sequentially, which was slow and struggled with long-range dependencies. Transformers replaced this with the **attention mechanism**, allowing the model to weigh the importance of different words in a sequence simultaneously—no matter how far apart they are.

This parallel processing made training much faster and enabled models to handle longer contexts effectively.

### Encoder-Decoder Structure

A full Transformer consists of two main components:
- **Encoder**: Processes the input text and creates a rich intermediate representation (embeddings) that captures meaning and context.
- **Decoder**: Generates output text step-by-step, attending both to the encoder's representation and to the tokens it has already generated.

Example: In machine translation, the encoder reads "I am a good dog." and the decoder generates "Je suis un bon chien." (French translation).

## Types of Large Language Models

Depending on which parts of the Transformer are used, LLMs fall into three categories:

### Encoder-Only Models
(e.g., BERT, RoBERTa)  
Best for understanding tasks like classification, named entity recognition, and question answering. They process text bidirectionally (seeing both left and right context).

### Decoder-Only Models
(e.g., GPT series, Llama, PaLM, Grok)  
Designed for generative tasks. They predict the next token autoregressively (left-to-right). Most popular modern LLMs used for chat and text generation are decoder-only.

### Encoder-Decoder Models
(e.g., T5, BART, original Transformer)  
Used for sequence-to-sequence tasks like translation, summarization, and advanced question answering.

## How LLMs Are Trained

### Pre-training
The model is trained on a massive, diverse text corpus using self-supervised learning (usually next-token prediction or masked language modeling). No human labels are needed—the data itself provides the supervision signal. This phase is extremely resource-intensive.

### Fine-tuning and Instruction Tuning
After pre-training:
- **Fine-tuning**: The model is further trained on smaller, task-specific datasets.
- **Instruction tuning / RLHF (Reinforcement Learning from Human Feedback)**: The model is aligned to follow instructions, be helpful, and avoid harmful outputs by using human preferences and feedback.

This two-stage process is what turns a raw pre-trained model into a capable assistant like modern chatbots.

With this foundation, LLMs can perform a wide variety of tasks—from writing essays to coding—often with just a prompt, without additional training.